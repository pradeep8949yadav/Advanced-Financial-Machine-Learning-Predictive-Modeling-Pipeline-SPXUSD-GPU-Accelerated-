{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e0f6284",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-16T05:17:27.532006Z",
     "iopub.status.busy": "2025-10-16T05:17:27.531786Z",
     "iopub.status.idle": "2025-10-16T05:17:32.904336Z",
     "shell.execute_reply": "2025-10-16T05:17:32.903769Z"
    },
    "papermill": {
     "duration": 5.379504,
     "end_time": "2025-10-16T05:17:32.905746",
     "exception": false,
     "start_time": "2025-10-16T05:17:27.526242",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CuPy active — 2 GPU(s)\n",
      "GPU 0: Tesla T4 | 15.8 GB\n",
      "GPU 1: Tesla T4 | 15.8 GB\n"
     ]
    }
   ],
   "source": [
    " # =========================\n",
    "# 0) SETUP — Imports & GPU\n",
    "# =========================\n",
    "import os, math, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple, List, Dict, Optional\n",
    "\n",
    "# Silence benign runtime warnings from comparisons on NaNs\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "# GPU (CuPy) optional\n",
    "try:\n",
    "    import cupy as cp\n",
    "    GPU_AVAILABLE = True\n",
    "    xp = cp\n",
    "    n_gpus = cp.cuda.runtime.getDeviceCount()\n",
    "    print(f\"CuPy active — {n_gpus} GPU(s)\")\n",
    "    for i in range(n_gpus):\n",
    "        props = cp.cuda.runtime.getDeviceProperties(i)\n",
    "        print(f\"GPU {i}: {props['name'].decode()} | {(props['totalGlobalMem']/1e9):.1f} GB\")\n",
    "except Exception:\n",
    "    GPU_AVAILABLE = False\n",
    "    xp = np\n",
    "    print(\"CuPy not available — using CPU (NumPy)\")\n",
    "\n",
    "# ADF test (optional)\n",
    "try:\n",
    "    from statsmodels.tsa.stattools import adfuller\n",
    "except Exception:\n",
    "    adfuller = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b28fa61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T05:17:32.915384Z",
     "iopub.status.busy": "2025-10-16T05:17:32.915039Z",
     "iopub.status.idle": "2025-10-16T05:17:33.186539Z",
     "shell.execute_reply": "2025-10-16T05:17:33.185759Z"
    },
    "papermill": {
     "duration": 0.277577,
     "end_time": "2025-10-16T05:17:33.187854",
     "exception": false,
     "start_time": "2025-10-16T05:17:32.910277",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /kaggle/input/stock-data-intraday-minute-bar\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"arashnic/stock-data-intraday-minute-bar\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4b86760",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T05:17:33.196406Z",
     "iopub.status.busy": "2025-10-16T05:17:33.196200Z",
     "iopub.status.idle": "2025-10-16T05:17:34.693477Z",
     "shell.execute_reply": "2025-10-16T05:17:34.692354Z"
    },
    "papermill": {
     "duration": 1.502827,
     "end_time": "2025-10-16T05:17:34.694700",
     "exception": false,
     "start_time": "2025-10-16T05:17:33.191873",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp    datetime64[ns, UTC]\n",
      "open                     float64\n",
      "high                     float64\n",
      "low                      float64\n",
      "close                    float64\n",
      "volume                     int64\n",
      "symbol                    object\n",
      "dtype: object\n",
      "range & shape: 2018-01-01 16:30:00+00:00 2018-12-31 16:13:00+00:00 (310381, 7)\n"
     ]
    }
   ],
   "source": [
    "# 1) LOAD & NORMALIZE SPXUSD minute CSV\n",
    "# ======================================\n",
    "FILE = \"/kaggle/input/stock-data-intraday-minute-bar/pyfinancialdata/data/stocks/histdata/SPXUSD/DAT_ASCII_SPXUSD_M1_2018.csv\"\n",
    "\n",
    "df = pd.read_csv(FILE, sep=';', header=None,\n",
    "                 names=['datetime','open','high','low','close','volume'])\n",
    "\n",
    "df['timestamp'] = pd.to_datetime(df['datetime'], format='%Y%m%d %H%M%S',\n",
    "                                 errors='coerce', utc=True)\n",
    "df['volume'] = pd.to_numeric(df['volume'], errors='coerce').fillna(0.0)\n",
    "df['symbol'] = 'SPXUSD'\n",
    "\n",
    "raw = (df.dropna(subset=['timestamp','open','high','low','close'])\n",
    "       .sort_values('timestamp')\n",
    "       .reset_index(drop=True)[['timestamp','open','high','low','close','volume','symbol']])\n",
    "\n",
    "print(raw.dtypes)\n",
    "print(\"range & shape:\", raw['timestamp'].min(), raw['timestamp'].max(), raw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f8d431a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T05:17:34.704045Z",
     "iopub.status.busy": "2025-10-16T05:17:34.703770Z",
     "iopub.status.idle": "2025-10-16T05:17:34.790006Z",
     "shell.execute_reply": "2025-10-16T05:17:34.789330Z"
    },
    "papermill": {
     "duration": 0.092191,
     "end_time": "2025-10-16T05:17:34.791105",
     "exception": false,
     "start_time": "2025-10-16T05:17:34.698914",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /kaggle/working/spxusd_m1_2018.feather\n"
     ]
    }
   ],
   "source": [
    "#2) SAVE to Feather\n",
    "# ======================\n",
    "OUT_DIR = \"/kaggle/working\"\n",
    "raw.to_feather(f\"{OUT_DIR}/spxusd_m1_2018.feather\")\n",
    "print(\"Saved:\", f\"{OUT_DIR}/spxusd_m1_2018.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36e46f06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T05:17:34.799453Z",
     "iopub.status.busy": "2025-10-16T05:17:34.799235Z",
     "iopub.status.idle": "2025-10-16T05:17:51.824772Z",
     "shell.execute_reply": "2025-10-16T05:17:51.823881Z"
    },
    "papermill": {
     "duration": 17.031014,
     "end_time": "2025-10-16T05:17:51.826000",
     "exception": false,
     "start_time": "2025-10-16T05:17:34.794986",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "volume bars shape: (156, 6)\n",
      "                  timestamp     open     high      low    close  volume\n",
      "0 2018-01-04 04:58:00+00:00  2668.00  2715.75  2668.00  2715.50       0\n",
      "1 2018-01-08 13:02:00+00:00  2715.25  2747.50  2713.00  2744.00       0\n",
      "2 2018-01-10 20:03:00+00:00  2744.13  2759.75  2736.50  2753.50       0\n",
      "3 2018-01-14 23:57:00+00:00  2753.25  2797.00  2747.75  2794.75       0\n",
      "4 2018-01-17 06:13:00+00:00  2794.88  2808.25  2769.25  2791.00       0\n"
     ]
    }
   ],
   "source": [
    "# 3.1) VOLUME BARS\n",
    "# ======================\n",
    "def _agg_rows_to_bar(rows: pd.DataFrame) -> pd.Series:\n",
    "    o = rows['open'].iloc[0]\n",
    "    h = rows['high'].max()\n",
    "    l = rows['low'].min()\n",
    "    c = rows['close'].iloc[-1]\n",
    "    v = rows['volume'].sum()\n",
    "    ts = rows['timestamp'].iloc[-1]\n",
    "    return pd.Series({'timestamp': ts, 'open': o, 'high': h, 'low': l, 'close': c, 'volume': v})\n",
    "\n",
    "def make_volume_bars(df: pd.DataFrame, threshold: float) -> pd.DataFrame:\n",
    "    out, cumv, bucket = [], 0.0, []\n",
    "    for _, row in df.iterrows():\n",
    "        cumv += float(row['volume'] if row['volume'] else 1.0)\n",
    "        bucket.append(row)\n",
    "        if cumv >= threshold:\n",
    "            out.append(_agg_rows_to_bar(pd.DataFrame(bucket)))\n",
    "            cumv, bucket = 0.0, []\n",
    "    if bucket:\n",
    "        out.append(_agg_rows_to_bar(pd.DataFrame(bucket)))\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "median_vol = max(1.0, raw['volume'].replace(0, np.nan).median(skipna=True) or 1.0)\n",
    "vol_bars = make_volume_bars(raw, threshold=2000 * median_vol)\n",
    "print(\"volume bars shape:\", vol_bars.shape)\n",
    "print(vol_bars.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b5a51f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T05:17:51.835274Z",
     "iopub.status.busy": "2025-10-16T05:17:51.835052Z",
     "iopub.status.idle": "2025-10-16T05:18:09.603792Z",
     "shell.execute_reply": "2025-10-16T05:18:09.602852Z"
    },
    "papermill": {
     "duration": 17.774935,
     "end_time": "2025-10-16T05:18:09.605245",
     "exception": false,
     "start_time": "2025-10-16T05:17:51.830310",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dollar bars shape: (63, 6)\n",
      "                  timestamp     open     high     low    close  volume\n",
      "0 2018-01-09 20:59:00+00:00  2668.00  2759.75  2668.0  2749.00       0\n",
      "1 2018-01-17 05:25:00+00:00  2748.88  2808.25  2736.5  2787.50       0\n",
      "2 2018-01-23 16:57:00+00:00  2787.38  2844.50  2780.5  2838.00       0\n",
      "3 2018-01-30 03:54:00+00:00  2837.88  2878.25  2825.5  2842.50       0\n",
      "4 2018-02-04 23:03:00+00:00  2842.25  2845.50  2733.0  2746.88       0\n"
     ]
    }
   ],
   "source": [
    "# ======================\n",
    "# 3.2) DOLLAR BARS\n",
    "# ======================\n",
    "def make_dollar_bars(df: pd.DataFrame, threshold: float) -> pd.DataFrame:\n",
    "    out, cumd, bucket = [], 0.0, []\n",
    "    for _, row in df.iterrows():\n",
    "        vol = float(row['volume'] if row['volume'] else 1.0)\n",
    "        dollar = float(row['close']) * vol\n",
    "        cumd += dollar\n",
    "        bucket.append(row)\n",
    "        if cumd >= threshold:\n",
    "            out.append(_agg_rows_to_bar(pd.DataFrame(bucket)))\n",
    "            cumd, bucket = 0.0, []\n",
    "    if bucket:\n",
    "        out.append(_agg_rows_to_bar(pd.DataFrame(bucket)))\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "median_close = float(raw['close'].median())\n",
    "dollar_bars = make_dollar_bars(raw, threshold=median_close * median_vol * 5000)\n",
    "print(\"dollar bars shape:\", dollar_bars.shape)\n",
    "print(dollar_bars.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15fd8229",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T05:18:09.615241Z",
     "iopub.status.busy": "2025-10-16T05:18:09.615023Z",
     "iopub.status.idle": "2025-10-16T05:20:09.889718Z",
     "shell.execute_reply": "2025-10-16T05:20:09.888958Z"
    },
    "papermill": {
     "duration": 120.285284,
     "end_time": "2025-10-16T05:20:09.895341",
     "exception": false,
     "start_time": "2025-10-16T05:18:09.610057",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info bars shape: (17, 6)\n",
      "                  timestamp     open     high      low    close  volume\n",
      "0 2018-02-05 15:12:00+00:00  2668.00  2878.25  2635.75  2669.75       0\n",
      "1 2018-02-05 16:13:00+00:00  2671.13  2696.50  2604.50  2620.75       0\n",
      "2 2018-02-05 20:58:00+00:00  2619.75  2634.00  2542.00  2551.25       0\n",
      "3 2018-02-06 07:26:00+00:00  2551.50  2644.25  2529.00  2602.25       0\n",
      "4 2018-02-06 09:32:00+00:00  2602.50  2614.38  2573.00  2614.25       0\n"
     ]
    }
   ],
   "source": [
    "# 3.3) INFORMATION (VOLATILITY) BARS (GPU)\n",
    "# ===========================================\n",
    "def make_volatility_bars(df: pd.DataFrame, sigma_thresh: float = 0.9, lookback: int = 60) -> pd.DataFrame:\n",
    "    out, bucket, rets = [], [], []\n",
    "    for _, row in df.iterrows():\n",
    "        if bucket:\n",
    "            prev_close = bucket[-1]['close']\n",
    "            rets.append(math.log((row['close'] + 1e-12)/(prev_close + 1e-12)))\n",
    "        bucket.append(row)\n",
    "        if len(rets) >= lookback:\n",
    "            r = xp.array(rets[-lookback:], dtype=float)\n",
    "            sigma = float(xp.std(r)) * math.sqrt(252*24*60)  # annualized\n",
    "            if sigma >= sigma_thresh:\n",
    "                out.append(_agg_rows_to_bar(pd.DataFrame(bucket)))\n",
    "                bucket, rets = [], []\n",
    "    if bucket:\n",
    "        out.append(_agg_rows_to_bar(pd.DataFrame(bucket)))\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "info_bars = make_volatility_bars(raw, sigma_thresh=0.9, lookback=60)\n",
    "print(\"info bars shape:\", info_bars.shape)\n",
    "print(info_bars.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "636c9d19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T05:20:09.905023Z",
     "iopub.status.busy": "2025-10-16T05:20:09.904681Z",
     "iopub.status.idle": "2025-10-16T05:25:11.071171Z",
     "shell.execute_reply": "2025-10-16T05:25:11.070401Z"
    },
    "papermill": {
     "duration": 301.177503,
     "end_time": "2025-10-16T05:25:11.077104",
     "exception": false,
     "start_time": "2025-10-16T05:20:09.899601",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin\n",
      " 1.0    76457\n",
      "-1.0    73413\n",
      " 0.0     4600\n",
      "Name: count, dtype: int64\n",
      "                                ret  bin      trgt                        t1\n",
      "timestamp                                                                   \n",
      "2018-01-03 11:48:00+00:00  0.002308  1.0  0.002146 2018-01-03 15:46:00+00:00\n",
      "2018-01-03 11:49:00+00:00  0.002032  1.0  0.001921 2018-01-03 15:38:00+00:00\n",
      "2018-01-03 11:50:00+00:00  0.001847  1.0  0.001760 2018-01-03 15:36:00+00:00\n",
      "2018-01-03 11:51:00+00:00  0.001662  1.0  0.001591 2018-01-03 12:51:00+00:00\n",
      "2018-01-03 11:52:00+00:00  0.001478  1.0  0.001477 2018-01-03 12:49:00+00:00\n"
     ]
    }
   ],
   "source": [
    "# 4) TRIPLE-BARRIER LABELING (final fixed)\n",
    "# ==============================================\n",
    "def get_daily_vol(close: pd.Series, span: int = 50, day_minutes: int = 1440) -> pd.Series:\n",
    "    prev = close.shift(day_minutes)\n",
    "    returns = (close / prev) - 1\n",
    "    return returns.ewm(span=span).std()\n",
    "\n",
    "def get_events(close: pd.Series, daily_vol: pd.Series, pt_sl: Tuple[float,float],\n",
    "               min_ret: float, num_minutes: int) -> pd.DataFrame:\n",
    "    trgt = daily_vol.copy()\n",
    "    trgt = trgt[trgt > min_ret].dropna()\n",
    "    t1 = trgt.index + pd.Timedelta(minutes=num_minutes)\n",
    "    return pd.DataFrame({'t1': t1, 'trgt': trgt})\n",
    "\n",
    "def apply_pt_sl_on_t1(close: pd.Series, events: pd.DataFrame, pt_sl: Tuple[float,float]) -> pd.DataFrame:\n",
    "    out = events[['t1']].copy()\n",
    "    pt, sl = pt_sl\n",
    "    for t0, e in events.iterrows():\n",
    "        if t0 not in close.index:\n",
    "            continue\n",
    "        p0 = close.loc[t0]\n",
    "        up = p0 * (1 + pt*e['trgt']) if pt > 0 else np.nan\n",
    "        dn = p0 * (1 - sl*e['trgt']) if sl > 0 else np.nan\n",
    "        path = close.loc[t0:e['t1']].dropna()\n",
    "\n",
    "        hit_up = path[path >= up].index.min() if pd.notna(up) and len(path) else pd.NaT\n",
    "        hit_dn = path[path <= dn].index.min() if pd.notna(dn) and len(path) else pd.NaT\n",
    "        first = min([x for x in [hit_up, hit_dn, e['t1']] if pd.notna(x)])\n",
    "\n",
    "        out.loc[t0, 't1'] = first\n",
    "        out.loc[t0, 'trgt'] = e['trgt']\n",
    "        out.loc[t0, 'up_price'] = up\n",
    "        out.loc[t0, 'dn_price'] = dn\n",
    "    return out\n",
    "\n",
    "def get_bins(close: pd.Series, events: pd.DataFrame, pt_sl: Tuple[float,float]) -> pd.DataFrame:\n",
    "    ev = apply_pt_sl_on_t1(close, events, pt_sl)\n",
    "    out = pd.DataFrame(index=ev.index)\n",
    "    for t0, e in ev.iterrows():\n",
    "        t1 = e['t1']\n",
    "        if pd.isna(t1) or t1 not in close.index:\n",
    "            out.loc[t0, 'ret'] = 0.0\n",
    "            out.loc[t0, 'bin'] = 0\n",
    "            continue\n",
    "\n",
    "        ret = (close.loc[t1] / close.loc[t0]) - 1\n",
    "        out.loc[t0, 'ret'] = ret\n",
    "        val = close.loc[t1]\n",
    "        if pd.notna(e['up_price']) and not pd.isna(val) and val >= e['up_price']:\n",
    "            out.loc[t0, 'bin'] = 1\n",
    "        elif pd.notna(e['dn_price']) and not pd.isna(val) and val <= e['dn_price']:\n",
    "            out.loc[t0, 'bin'] = -1\n",
    "        else:\n",
    "            out.loc[t0, 'bin'] = 0\n",
    "    return out.join(ev[['trgt','t1']])\n",
    "\n",
    "# Apply on SPXUSD close series\n",
    "close = raw.set_index(pd.DatetimeIndex(raw['timestamp']))['close'].sort_index()\n",
    "dvol = get_daily_vol(close, span=50, day_minutes=1440)\n",
    "min_ret = float(dvol.median())\n",
    "events = get_events(close, dvol, pt_sl=(1,1), min_ret=min_ret, num_minutes=720)  # 12h\n",
    "bins = get_bins(close, events, pt_sl=(1,1))\n",
    "\n",
    "print(bins['bin'].value_counts(dropna=False))\n",
    "print(bins.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25cb2271",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T05:25:11.086222Z",
     "iopub.status.busy": "2025-10-16T05:25:11.085995Z",
     "iopub.status.idle": "2025-10-16T05:25:11.754180Z",
     "shell.execute_reply": "2025-10-16T05:25:11.753493Z"
    },
    "papermill": {
     "duration": 0.674429,
     "end_time": "2025-10-16T05:25:11.755883",
     "exception": false,
     "start_time": "2025-10-16T05:25:11.081454",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: (154470, 5) (154470,) index_equal: True\n",
      "primary_probs: (46341,)\n"
     ]
    }
   ],
   "source": [
    "# 4.5) PRIMARY MODEL → produces p̂_i = P(y=+1 | X)\n",
    "#     (simple baseline using raw OHLCV features for demo)\n",
    "# ===========================================================\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# align features (X) with label timestamps (bins.index)\n",
    "raw_idx = raw.set_index('timestamp').sort_index()\n",
    "bins = bins.sort_index()\n",
    "\n",
    "X = raw_idx.loc[bins.index, ['open','high','low','close','volume']]\n",
    "y = bins['bin'].fillna(0)           # true labels in {-1,0,+1}\n",
    "y_model = y.replace(-1, 0)          # convert to {0,1} for logistic regression\n",
    "\n",
    "print(\"Shapes:\", X.shape, y_model.shape, \"index_equal:\", X.index.equals(y_model.index))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_model, test_size=0.3, shuffle=False)\n",
    "\n",
    "primary = LogisticRegression(max_iter=1000)\n",
    "primary.fit(X_train, y_train)\n",
    "\n",
    "# p̂_i (probability of UP = class 1)\n",
    "primary_probs = pd.Series(primary.predict_proba(X_test)[:,1], index=y_test.index)\n",
    "print(\"primary_probs:\", primary_probs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90982d73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T05:25:11.773111Z",
     "iopub.status.busy": "2025-10-16T05:25:11.772867Z",
     "iopub.status.idle": "2025-10-16T05:25:11.791361Z",
     "shell.execute_reply": "2025-10-16T05:25:11.790722Z"
    },
    "papermill": {
     "duration": 0.029715,
     "end_time": "2025-10-16T05:25:11.793031",
     "exception": false,
     "start_time": "2025-10-16T05:25:11.763316",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta distribution:\n",
      " 1    0.511599\n",
      "0    0.488401\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 5) META-LABELING\n",
    "# =========================\n",
    "def meta_label(primary_probs: pd.Series, primary_labels: pd.Series, prob_thresh: float = 0.5) -> pd.Series:\n",
    "    y_true = (primary_labels > 0).astype(int)      # +1 -> 1, else 0\n",
    "    y_hat = (primary_probs >= prob_thresh).astype(int)\n",
    "    return (y_true == y_hat).astype(int)\n",
    "\n",
    "meta = meta_label(primary_probs, y.loc[primary_probs.index], prob_thresh=0.5)\n",
    "print(\"meta distribution:\\n\", meta.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f85fc412",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T05:25:11.808658Z",
     "iopub.status.busy": "2025-10-16T05:25:11.808412Z",
     "iopub.status.idle": "2025-10-16T05:25:11.815376Z",
     "shell.execute_reply": "2025-10-16T05:25:11.814773Z"
    },
    "papermill": {
     "duration": 0.016067,
     "end_time": "2025-10-16T05:25:11.816697",
     "exception": false,
     "start_time": "2025-10-16T05:25:11.800630",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Map each event [t0, t1] to integer indices over close_index (inclusive)\n",
    "def _events_to_spans(events: pd.DataFrame, close_index: pd.DatetimeIndex):\n",
    "    t0 = events.index\n",
    "    t1 = events['t1'].reindex(t0)\n",
    "    s = np.searchsorted(close_index.values, t0.values)\n",
    "    e = np.searchsorted(close_index.values, t1.values, side='right') - 1  # inclusive\n",
    "    # clip to index range & drop invalid\n",
    "    valid = (s >= 0) & (e >= s) & (e < len(close_index))\n",
    "    s, e = s[valid], e[valid]\n",
    "    idx = t0.values[valid]\n",
    "    lengths = (e - s + 1).astype(np.int64)\n",
    "    inv_len = 1.0 / lengths\n",
    "    return idx, s, e, lengths, inv_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de67e8a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T05:25:11.830881Z",
     "iopub.status.busy": "2025-10-16T05:25:11.830659Z",
     "iopub.status.idle": "2025-10-16T05:25:11.836530Z",
     "shell.execute_reply": "2025-10-16T05:25:11.835960Z"
    },
    "papermill": {
     "duration": 0.014255,
     "end_time": "2025-10-16T05:25:11.837890",
     "exception": false,
     "start_time": "2025-10-16T05:25:11.823635",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build an array of concurrency counts for a set of events using prefix-sum (O(T + |set|))\n",
    "def _concurrency_from_spans(T: int, starts: np.ndarray, ends: np.ndarray) -> np.ndarray:\n",
    "    diff = np.zeros(T + 1, dtype=np.int32)\n",
    "    np.add.at(diff, starts, 1)\n",
    "    np.add.at(diff, ends + 1, -1)\n",
    "    w = np.cumsum(diff[:-1])  # shape (T,)\n",
    "    return w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b76c9c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T05:25:11.852394Z",
     "iopub.status.busy": "2025-10-16T05:25:11.852186Z",
     "iopub.status.idle": "2025-10-16T05:25:11.858361Z",
     "shell.execute_reply": "2025-10-16T05:25:11.857855Z"
    },
    "papermill": {
     "duration": 0.015046,
     "end_time": "2025-10-16T05:25:11.859953",
     "exception": false,
     "start_time": "2025-10-16T05:25:11.844907",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _mean_uniqueness_for_set(T, starts, ends):\n",
    "    # concurrency\n",
    "    w = _concurrency_from_spans(T, starts, ends)\n",
    "    # avoid division by zero; we only query within active regions anyway\n",
    "    b = np.zeros_like(w, dtype=float)\n",
    "    nz = w > 0\n",
    "    b[nz] = 1.0 / w[nz]             # b[t] = 1 / concurrency\n",
    "    csb = np.cumsum(b)               # prefix sums for fast segment sums\n",
    "\n",
    "    lengths = (ends - starts + 1).astype(np.int64)\n",
    "    seg_sum = csb[ends] - np.where(starts > 0, csb[starts - 1], 0.0)\n",
    "    u = seg_sum / lengths            # per-event uniqueness\n",
    "    return u, w, b, csb, lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e73b07a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T05:25:11.871873Z",
     "iopub.status.busy": "2025-10-16T05:25:11.871462Z",
     "iopub.status.idle": "2025-10-16T05:25:11.881763Z",
     "shell.execute_reply": "2025-10-16T05:25:11.881290Z"
    },
    "papermill": {
     "duration": 0.016093,
     "end_time": "2025-10-16T05:25:11.882782",
     "exception": false,
     "start_time": "2025-10-16T05:25:11.866689",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sequential_bootstrap_fast(events: pd.DataFrame,\n",
    "                              close_index: pd.DatetimeIndex,\n",
    "                              size: int) -> List[pd.Timestamp]:\n",
    "    # ---------- preprocess once ----------\n",
    "    idx, s_all, e_all, lengths_all, invlen_all = _events_to_spans(events, close_index)\n",
    "    T = len(close_index)\n",
    "    N = len(idx)\n",
    "    if N == 0:\n",
    "        return []\n",
    "\n",
    "    # containers for the selected set (by integer positions)\n",
    "    chosen_mask = np.zeros(N, dtype=bool)\n",
    "    chosen = []              # list of timestamps to return\n",
    "    chosen_starts = []\n",
    "    chosen_ends   = []\n",
    "    chosen_invlen = []\n",
    "\n",
    "    # maintain current totals\n",
    "    total_metric = 0.0       # = sum of mean uniqueness over selected events\n",
    "    k = 0                    # |S|\n",
    "\n",
    "    # Precompute arrays we’ll need for quick unions\n",
    "    # (we will rebuild per-iteration arrays from chosen spans with prefix sums)\n",
    "    while k < size and k < N:\n",
    "        # ---------- build per-iteration arrays from current chosen set ----------\n",
    "        if k == 0:\n",
    "            # no selected events yet\n",
    "            w = np.zeros(T, dtype=int)\n",
    "            b = np.zeros(T, dtype=float)\n",
    "            csA = csWD = None    # will not be used until k>=1\n",
    "            total_metric = 0.0\n",
    "            # for wlen (sum of 1/len_i over active events), zero array\n",
    "            wlen = np.zeros(T, dtype=float)\n",
    "        else:\n",
    "            # concurrency counts for selected\n",
    "            sel_s = np.asarray(chosen_starts)\n",
    "            sel_e = np.asarray(chosen_ends)\n",
    "            w = _concurrency_from_spans(T, sel_s, sel_e)\n",
    "\n",
    "            # wlen[t] = sum of 1/len_i over active selected events\n",
    "            diff = np.zeros(T + 1, dtype=float)\n",
    "            np.add.at(diff, sel_s, chosen_invlen)\n",
    "            np.add.at(diff, sel_e + 1, -np.array(chosen_invlen))\n",
    "            wlen = np.cumsum(diff[:-1])\n",
    "\n",
    "            # b = 1/w on active bars\n",
    "            b = np.zeros(T, dtype=float)\n",
    "            nz = w > 0\n",
    "            b[nz] = 1.0 / w[nz]\n",
    "            # a = 1/(w+1) everywhere\n",
    "            a = 1.0 / (w + 1.0)\n",
    "            d = np.zeros(T, dtype=float)           # d = a - b where w>0, else 0\n",
    "            d[nz] = a[nz] - b[nz]\n",
    "\n",
    "            # prefix sums for fast segment queries\n",
    "            csA  = np.cumsum(a)                    # for candidate's own uniqueness\n",
    "            csWD = np.cumsum(wlen * d)             # for selected set adjustment\n",
    "\n",
    "            # recompute current total_metric (= sum of mean uniqueness of selected)\n",
    "            # vectorized using prefix sums of b\n",
    "            csB = np.cumsum(b)\n",
    "            segB = csB[sel_e] - np.where(sel_s > 0, csB[sel_s - 1], 0.0)\n",
    "            total_metric = float((segB * chosen_invlen).sum())\n",
    "\n",
    "        # ---------- evaluate candidates in O(1) each ----------\n",
    "        not_sel = ~chosen_mask\n",
    "        cand_idx = np.nonzero(not_sel)[0]\n",
    "        if len(cand_idx) == 0:\n",
    "            break\n",
    "\n",
    "        if k == 0:\n",
    "            # first pick: choose the event with highest own uniqueness under w=0 → a=1\n",
    "            # uniqueness = 1.0 over its active bars, so tie-break by longest? use max length.\n",
    "            j = cand_idx[np.argmax(lengths_all[cand_idx])]\n",
    "            best = j\n",
    "        else:\n",
    "            # compute per-candidate scores\n",
    "            s_c = s_all[cand_idx]\n",
    "            e_c = e_all[cand_idx]\n",
    "            len_c = lengths_all[cand_idx]\n",
    "            invlen_c = 1.0 / len_c\n",
    "\n",
    "            # candidate's own sum of a over its range\n",
    "            sumA = csA[e_c] - np.where(s_c > 0, csA[s_c - 1], 0.0)\n",
    "            u_c = sumA * invlen_c\n",
    "\n",
    "            # adjustment to selected totals over overlap range: sum(wlen * d)\n",
    "            adj = csWD[e_c] - np.where(s_c > 0, csWD[s_c - 1], 0.0)\n",
    "\n",
    "            # new average if we add candidate: (total_metric + adj + u_c) / (k+1)\n",
    "            score = (total_metric + adj + u_c) / (k + 1.0)\n",
    "\n",
    "            best = cand_idx[np.argmax(score)]\n",
    "\n",
    "        # ---------- commit best candidate ----------\n",
    "        chosen_mask[best] = True\n",
    "        chosen.append(idx[best])\n",
    "        chosen_starts.append(s_all[best])\n",
    "        chosen_ends.append(e_all[best])\n",
    "        chosen_invlen.append(1.0 / lengths_all[best])\n",
    "        k += 1\n",
    "\n",
    "    # return timestamps of chosen events in selection order\n",
    "    # (same type as events.index)\n",
    "    return list(chosen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9aede3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T05:25:11.891426Z",
     "iopub.status.busy": "2025-10-16T05:25:11.891263Z",
     "iopub.status.idle": "2025-10-16T05:25:14.567141Z",
     "shell.execute_reply": "2025-10-16T05:25:14.566282Z"
    },
    "papermill": {
     "duration": 2.681615,
     "end_time": "2025-10-16T05:25:14.568335",
     "exception": false,
     "start_time": "2025-10-16T05:25:11.886720",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg uniqueness (all events): 0.0034299971931661143\n",
      "Sequential bootstrap picked: 200\n"
     ]
    }
   ],
   "source": [
    "# Precompute average uniqueness for all events (optional, for reporting)\n",
    "idx, s_all, e_all, lengths_all, invlen_all = _events_to_spans(events, close.index)\n",
    "u_all, *_ = _mean_uniqueness_for_set(len(close.index), s_all, e_all)\n",
    "print(\"Avg uniqueness (all events):\", float(np.mean(u_all)))\n",
    "\n",
    "# Fast sequential bootstrap\n",
    "chosen = sequential_bootstrap_fast(events, close.index, size=min(200, len(events)))\n",
    "print(\"Sequential bootstrap picked:\", len(chosen))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344d193d",
   "metadata": {
    "papermill": {
     "duration": 0.004048,
     "end_time": "2025-10-16T05:25:14.576784",
     "exception": false,
     "start_time": "2025-10-16T05:25:14.572736",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Perfect 👏 — that’s *exactly* what you want to see.\n",
    "\n",
    "Let’s unpack those results clearly so you understand what those two numbers *mean* and why they matter:\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ 1️⃣ “Average uniqueness (all events): 0.0034…”\n",
    "\n",
    "This tells you:\n",
    "\n",
    "> On average, each event (trade or label) overlaps **heavily** with many others.\n",
    "\n",
    "Because uniqueness ( u_i = \\text{mean}(1 / \\text{concurrency}) ).\n",
    "\n",
    "So if `avg uniqueness ≈ 0.0034`, then:\n",
    "\n",
    "[\n",
    "\\text{average concurrency} ≈ \\frac{1}{0.0034} ≈ 294\n",
    "]\n",
    "\n",
    "That means at any given timestamp, about **~294 events** are active on average.\n",
    "\n",
    "🧠 **Interpretation:**\n",
    "\n",
    "* The dataset is very dense — events overlap a lot in time.\n",
    "* This is *normal* in minute-level or tick data (many trades are active simultaneously).\n",
    "* High concurrency = low uniqueness → potential label leakage if you sample randomly.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ 2️⃣ “Sequential bootstrap picked: 200”\n",
    "\n",
    "That means your **greedy uniqueness-optimized sampler** successfully chose **200 events** (out of possibly thousands) that are:\n",
    "\n",
    "* The **most independent** possible given the overlap structure,\n",
    "* **Spread out** in time,\n",
    "* And thus give your model a **clean, low-correlation** training subset.\n",
    "\n",
    "Each event selected contributes **maximum marginal uniqueness** to the group — i.e., it adds new, non-redundant information rather than reinforcing already overlapping ones.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 How to Think About It Visually\n",
    "\n",
    "Imagine your timeline looks like this:\n",
    "\n",
    "```\n",
    "|--- Event 1 ---|\n",
    "    |--- Event 2 ---|\n",
    "        |--- Event 3 ---|\n",
    "                        |--- Event 4 ---|\n",
    "```\n",
    "\n",
    "* If you train on 1, 2, 3 → they overlap → model sees almost the same info three times.\n",
    "* Sequential bootstrap instead might pick 1 and 4 → **minimal overlap** → more diverse signal.\n",
    "\n",
    "So the algorithm has **filtered out redundancy**, keeping the “clean” signals.\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 Practical Use Now\n",
    "\n",
    "You can use those `chosen` timestamps to **subset your labels and features** before training:\n",
    "\n",
    "```python\n",
    "chosen_events = events.loc[chosen]\n",
    "X_train = X.loc[chosen_events.index]\n",
    "y_train = y.loc[chosen_events.index]\n",
    "```\n",
    "\n",
    "This gives you a **high-quality training sample**:\n",
    "\n",
    "* Labels are non-overlapping (independent in time).\n",
    "* Features can now be **fractionally differentiated (FFD)** for stationarity.\n",
    "* You avoid the double trap of temporal dependence **in both rows and columns**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔁 Recap of What You’ve Achieved So Far\n",
    "\n",
    "| Step                                         | Purpose                                     | Outcome                        |\n",
    "| -------------------------------------------- | ------------------------------------------- | ------------------------------ |\n",
    "| **Concurrency**                              | Count how many events overlap at each time  | Baseline overlap map           |\n",
    "| **Uniqueness**                               | Measure independence per event              | Uniqueness scores              |\n",
    "| **Sequential Bootstrap (fast)**              | Select subset maximizing average uniqueness | 200 clean, independent samples |\n",
    "| **→ Next: Fractional Differentiation (FFD)** | Make features stationary but memory-rich    | Ready for modeling             |\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ Next Move\n",
    "\n",
    "Now that your **labels** (rows) are independent,\n",
    "the next step is to make your **features** stationary:\n",
    "\n",
    "```python\n",
    "ffd_close = fracdiff_ffd_gpu(close, d=0.4, thres=1e-4)\n",
    "print(adf_stat(ffd_close))\n",
    "```\n",
    "\n",
    "Then train on:\n",
    "\n",
    "```python\n",
    "X_ffd = X.assign(close_ffd=ffd_close)\n",
    "X_train = X_ffd.loc[chosen_events.index]\n",
    "y_train = y.loc[chosen_events.index]\n",
    "```\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ede7faf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T05:25:14.585944Z",
     "iopub.status.busy": "2025-10-16T05:25:14.585704Z",
     "iopub.status.idle": "2025-10-16T05:27:57.853430Z",
     "shell.execute_reply": "2025-10-16T05:27:57.852748Z"
    },
    "papermill": {
     "duration": 163.278229,
     "end_time": "2025-10-16T05:27:57.859002",
     "exception": false,
     "start_time": "2025-10-16T05:25:14.580773",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-NaN raw: 310381 | Non-NaN FFD: 309197\n",
      "ADF raw: {'adf_stat': -1.7753650647015773, 'pvalue': 0.39275900851346823}\n",
      "ADF ffd: {'adf_stat': -5.283443060842624, 'pvalue': 5.925045819499616e-06}\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 7) FRACTIONAL DIFFERENTIATION (FFD) + ADF — FAST (No Python loops)\n",
    "# ============================================================\n",
    "def get_weights_ffd(d: float, thres: float = 1e-5, max_size: int = 10000) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Fixed-width fractional differentiation weights.\n",
    "    Returns weights oldest->newest (length = window size).\n",
    "    \"\"\"\n",
    "    w = [1.0]\n",
    "    k = 1\n",
    "    while k < max_size:\n",
    "        w_ = -w[-1] * (d - k + 1) / k\n",
    "        if abs(w_) < thres:\n",
    "            break\n",
    "        w.append(w_)\n",
    "        k += 1\n",
    "    # Oldest to newest\n",
    "    return np.array(w[::-1], dtype=float)\n",
    "\n",
    "\n",
    "def fracdiff_ffd_fast(series: pd.Series, d: float, thres: float = 1e-5) -> pd.Series:\n",
    "    \"\"\"\n",
    "    FAST fixed-width fractional differencing using vectorized correlation.\n",
    "    - Works on GPU if CuPy is available (xp = cp).\n",
    "    - Handles NaNs: any window containing NaN → output NaN for that position.\n",
    "    \"\"\"\n",
    "    # 1) Weights (CPU) → move to xp\n",
    "    w = get_weights_ffd(d, thres=thres)\n",
    "    width = int(len(w))\n",
    "    if width == 0:\n",
    "        return pd.Series(np.full(series.shape, np.nan), index=series.index)\n",
    "\n",
    "    w_xp = xp.asarray(w, dtype=float)\n",
    "\n",
    "    # 2) Data & mask to xp\n",
    "    vals_np = series.values.astype(float)\n",
    "    mask_np = ~np.isnan(vals_np)  # True where valid\n",
    "    vals_filled_np = np.where(mask_np, vals_np, 0.0)\n",
    "\n",
    "    vals = xp.asarray(vals_filled_np, dtype=float)\n",
    "    mask = xp.asarray(mask_np.astype(float))  # 1.0 valid, 0.0 NaN\n",
    "\n",
    "    # 3) Vectorized \"sliding dot\": use correlate so we don't need to flip weights\n",
    "    #    correlate(vals, w)[i] = sum(vals[i:i+width] * w)\n",
    "    num = xp.correlate(vals, w_xp, mode='valid')              # shape: (n - width + 1,)\n",
    "\n",
    "    # 4) Validity via mask: count of valid points in each window\n",
    "    ones = xp.ones(width, dtype=float)\n",
    "    cnt = xp.correlate(mask, ones, mode='valid')              # number of non-NaN per window\n",
    "\n",
    "    # 5) Stitch back into full-length array and null-out invalid windows\n",
    "    out = xp.full(vals.shape, xp.nan, dtype=float)\n",
    "    valid_slice = slice(width - 1, len(vals))                 # positions aligned to end of each window\n",
    "    out_valid = num                                           # same shape as cnt\n",
    "    # Only keep windows with full validity (cnt == width)\n",
    "    valid_windows = cnt == float(width)\n",
    "    out_valid = xp.where(valid_windows, out_valid, xp.nan)\n",
    "    out[valid_slice] = out_valid\n",
    "\n",
    "    # 6) Back to CPU for pandas\n",
    "    out_cpu = xp.asnumpy(out) if GPU_AVAILABLE else out\n",
    "    return pd.Series(out_cpu, index=series.index)\n",
    "\n",
    "\n",
    "def adf_stat(series: pd.Series, maxlag: Optional[int] = None) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Augmented Dickey–Fuller test wrapper.\n",
    "    \"\"\"\n",
    "    if adfuller is None:\n",
    "        return {\"adf_stat\": np.nan, \"pvalue\": np.nan}\n",
    "\n",
    "    s = series.dropna()\n",
    "    if len(s) < 40:\n",
    "        return {\"adf_stat\": np.nan, \"pvalue\": np.nan}\n",
    "\n",
    "    stat, p, *_ = adfuller(s, maxlag=maxlag, autolag=\"AIC\")\n",
    "    return {\"adf_stat\": float(stat), \"pvalue\": float(p)}\n",
    "\n",
    "\n",
    "# =======================\n",
    "# Apply FFD + ADF (FAST)\n",
    "# =======================\n",
    "ffd = fracdiff_ffd_fast(close, d=0.4, thres=1e-4)\n",
    "\n",
    "print(f\"Non-NaN raw: {close.dropna().shape[0]} | Non-NaN FFD: {ffd.dropna().shape[0]}\")\n",
    "print(\"ADF raw:\", adf_stat(close))\n",
    "print(\"ADF ffd:\", adf_stat(ffd))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcd9acb",
   "metadata": {
    "papermill": {
     "duration": 0.003918,
     "end_time": "2025-10-16T05:27:57.867214",
     "exception": false,
     "start_time": "2025-10-16T05:27:57.863296",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Exactly 🔥 — that’s a **perfect observation** and you’re absolutely right.\n",
    "\n",
    "Let’s unpack what’s going on here so you understand it at a deep, quantitative level.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ 1️⃣ What Happened\n",
    "\n",
    "You started with:\n",
    "\n",
    "```\n",
    "ADF raw: p-value = 0.3927  →  non-stationary\n",
    "```\n",
    "\n",
    "Then after fractional differentiation (`d = 0.4`):\n",
    "\n",
    "```\n",
    "ADF ffd: p-value = 0.0000059  →  stationary ✅\n",
    "```\n",
    "\n",
    "That means — in statistical terms —\n",
    "you effectively **“differentiated” the series just enough** to make it stationary,\n",
    "but **not as aggressively** as a full first difference (which would remove all memory).\n",
    "\n",
    "So yes, you can think of this as **“differentiating the price series once — but fractionally.”**\n",
    "It’s the middle ground between:\n",
    "\n",
    "| Method                | Differencing Order | Memory Retention | Stationarity     |\n",
    "| :-------------------- | :----------------- | :--------------- | :--------------- |\n",
    "| Raw prices            | d = 0.0            | High             | ❌ Non-stationary |\n",
    "| First difference      | d = 1.0            | None             | ✅ Stationary     |\n",
    "| Fractional difference | d = 0.4            | Partial          | ✅ Stationary     |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 2️⃣ Why We Don’t “Just Differentiate Once”\n",
    "\n",
    "When you difference once (`d=1`):\n",
    "\n",
    "* You lose all long-term information (trends, mean reversion structure).\n",
    "* Your model can only learn from very short-term fluctuations (like noise).\n",
    "* Signal-to-noise ratio drops dramatically.\n",
    "\n",
    "Fractional differentiation (here `d=0.4`) gives the **best of both worlds**:\n",
    "\n",
    "* Stationarity (confirmed by the very low p-value)\n",
    "* Preserved memory (unlike full differencing)\n",
    "\n",
    "So, you “differentiated one more time,”\n",
    "but only **by 40% of a full difference** — enough to stabilize, not destroy, the signal.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ 3️⃣ What the Numbers Tell Us\n",
    "\n",
    "Let’s interpret them quantitatively:\n",
    "\n",
    "| Metric              |    Value | Interpretation                                          |\n",
    "| :------------------ | -------: | :------------------------------------------------------ |\n",
    "| **ADF raw stat**    |   -1.775 | Too close to 0 → non-stationary                         |\n",
    "| **ADF raw p-value** |   0.3928 | > 0.05 → fails stationarity test                        |\n",
    "| **ADF ffd stat**    |   -5.283 | Far below critical value (~-2.86) → strongly stationary |\n",
    "| **ADF ffd p-value** | 5.9×10⁻⁶ | ≪ 0.05 → reject unit-root hypothesis ✅                  |\n",
    "\n",
    "So the fractional differentiation **successfully transformed** your price series from a random walk into a *mean-reverting* stationary series.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 4️⃣ Visual Intuition\n",
    "\n",
    "Here’s how the transformations compare conceptually:\n",
    "\n",
    "```\n",
    "Raw prices (non-stationary)\n",
    "   ↑\n",
    "   |          ↗ long upward drift ↗\n",
    "   |\n",
    "   +-------------------------------------------------------> time\n",
    "\n",
    "First difference (d = 1.0)\n",
    "   ↑\n",
    "   |  fluctuates around 0, but pure noise\n",
    "   +-------------------------------------------------------> time\n",
    "\n",
    "Fractional diff (d = 0.4)\n",
    "   ↑\n",
    "   |  mean-reverting & smoother, still tracks long swings\n",
    "   +-------------------------------------------------------> time\n",
    "```\n",
    "\n",
    "You “tempered” the drift without erasing the useful low-frequency structure.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧭 5️⃣ Why This Matters for You\n",
    "\n",
    "Now your **features (columns)** are statistically valid for model training:\n",
    "\n",
    "* They don’t have spurious correlations due to non-stationarity.\n",
    "* You can safely use them in regression, logistic, or ML pipelines.\n",
    "* You can generate lagged versions, returns, or volatility proxies based on `ffd`.\n",
    "\n",
    "Combined with your **Sequential Bootstrap** (which handled independence in labels),\n",
    "you now have a dataset that’s:\n",
    "\n",
    "* **Row-wise independent**\n",
    "* **Column-wise stationary**\n",
    "\n",
    "That’s the holy grail of quantitative modeling.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ 6️⃣ TL;DR — What You Achieved\n",
    "\n",
    "| Step                                 | Goal                                         | Result                                 |\n",
    "| ------------------------------------ | -------------------------------------------- | -------------------------------------- |\n",
    "| Fractional Differentiation (`d=0.4`) | Remove non-stationarity                      | ✅ Achieved                             |\n",
    "| ADF test                             | Verify statistical stationarity              | ✅ p ≈ 0.0000059                        |\n",
    "| Data retention                       | Avoid information loss                       | ✅ 99.6% of data kept                   |\n",
    "| Effect                               | You “differentiated once — but fractionally” | ✅ Stationary, memory-preserving series |\n",
    "\n",
    "---\n",
    "\n",
    " \n",
    "\n",
    "* raw prices\n",
    "* first differences\n",
    "* fractional differences (d = 0.4)\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5a19183",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T05:27:57.876421Z",
     "iopub.status.busy": "2025-10-16T05:27:57.875882Z",
     "iopub.status.idle": "2025-10-16T05:27:59.990656Z",
     "shell.execute_reply": "2025-10-16T05:27:59.989546Z"
    },
    "papermill": {
     "duration": 2.120922,
     "end_time": "2025-10-16T05:27:59.992152",
     "exception": false,
     "start_time": "2025-10-16T05:27:57.871230",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== OUTPUTS SAVED TO /kaggle/working ===\n",
      "\n",
      "Label distribution:\n",
      "bin\n",
      " 1.0    0.494963\n",
      "-1.0    0.475257\n",
      " 0.0    0.029779\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Bars shapes:\n",
      "Volume: (156, 6) Dollar: (63, 6) Info: (17, 6)\n"
     ]
    }
   ],
   "source": [
    "# 8) SAVE ARTIFACTS & QUICK SUMMARY\n",
    "# ======================================\n",
    "OUT = \"/kaggle/working\"\n",
    "vol_bars.to_feather(f\"{OUT}/volume_bars.feather\")\n",
    "dollar_bars.to_feather(f\"{OUT}/dollar_bars.feather\")\n",
    "info_bars.to_feather(f\"{OUT}/info_bars.feather\")\n",
    "bins.to_csv(f\"{OUT}/triple_barrier_labels.csv\", index=True)\n",
    "\n",
    "print(\"=== OUTPUTS SAVED TO\", OUT, \"===\")\n",
    "print(\"\\nLabel distribution:\")\n",
    "print(bins['bin'].value_counts(normalize=True, dropna=False))\n",
    "print(\"\\nBars shapes:\")\n",
    "print(\"Volume:\", vol_bars.shape, \"Dollar:\", dollar_bars.shape, \"Info:\", info_bars.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3056ff90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T05:28:00.002919Z",
     "iopub.status.busy": "2025-10-16T05:28:00.002178Z",
     "iopub.status.idle": "2025-10-16T05:30:41.626335Z",
     "shell.execute_reply": "2025-10-16T05:30:41.625668Z"
    },
    "papermill": {
     "duration": 161.634962,
     "end_time": "2025-10-16T05:30:41.631968",
     "exception": false,
     "start_time": "2025-10-16T05:27:59.997006",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Markdown summary: /kaggle/working/project1_summary.md\n"
     ]
    }
   ],
   "source": [
    "# ======================================\n",
    "# 9) OPTIONAL — AUTO MARKDOWN REPORT\n",
    "# ======================================\n",
    "summary_md = f\"\"\"\n",
    "# Project 1 — SPXUSD (GPU-aware)\n",
    "\n",
    "**Bars:**\n",
    "- Volume: {vol_bars.shape[0]}\n",
    "- Dollar: {dollar_bars.shape[0]}\n",
    "- Info (vol): {info_bars.shape[0]}\n",
    "\n",
    "**Triple-Barrier label distribution:**\n",
    "{bins['bin'].value_counts(normalize=True).to_string()}\n",
    "\n",
    "**ADF:**\n",
    "- Raw p-value: {adf_stat(close)['pvalue']:.4f}\n",
    "- FFD p-value: {adf_stat(ffd)['pvalue']:.4f}\n",
    "\n",
    "**Primary → Meta:**\n",
    "- Primary model: LogisticRegression\n",
    "- Meta (share of 1s): {meta.mean():.4f}\n",
    "\n",
    "**GPU:**\n",
    "- Enabled: {'Yes' if GPU_AVAILABLE else 'No'}\n",
    "- Backend: {'CuPy' if GPU_AVAILABLE else 'NumPy'}\n",
    "\"\"\"\n",
    "\n",
    "with open(f\"{OUT}/project1_summary.md\", \"w\") as f:\n",
    "    f.write(summary_md)\n",
    "\n",
    "print(\"Markdown summary:\", f\"{OUT}/project1_summary.md\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 1935576,
     "datasetId": 1130576,
     "isSourceIdPinned": false,
     "sourceId": 1897283,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 798.849964,
   "end_time": "2025-10-16T05:30:42.353893",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-16T05:17:23.503929",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
